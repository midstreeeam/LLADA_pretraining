wandb:
  entity: null
  resume: 'auto'

experiment:
  project: "llada-tiny-autodl"
  name: "llada-tiny-autodl"
  output_dir: "outputs/tiny_test_autodl"
  save_every: 10000
  eval_every: 2000
  generate_every: 2000
  log_every: 200
  log_grad_norm_every: 200
  checkpoints_total_limit: 3
  resume_from_checkpoint: "outputs/tiny_test_autodl/checkpoint-240000/"
  reset_optimizer: false

model:
  pretrained_model_path: "gpt2"
  llada_config:
    gradient_checkpointing: false
    d_model: 768
    n_heads: 12
    n_layers: 12
    n_kv_heads: 12
    mlp_ratio: 4
    max_sequence_length: 512
    vocab_size: 50257
    new_vocab_size: 50304
    rope: true
    rope_theta: 10000.0
    flash_attention: false
    attention_dropout: 0.0
    residual_dropout: 0.0
    embedding_dropout: 0.0
    layer_norm_type: "rms"
    activation_type: "swiglu"
    init_fn: "normal"
    init_std: 0.02
    weight_tying: true
    include_bias: false

dataset:
  params:
    train_shards_path_or_url: "data/tinystories_train.jsonl"
    shuffle_buffer_size: 500
    num_workers: 2
    pin_memory: true
    persistent_workers: false
  preprocessing:
    max_seq_length: 128

optimizer:
  name: adamw
  params:
    learning_rate: 5e-4
    scale_lr: false
    beta1: 0.9
    beta2: 0.95
    weight_decay: 0.01
    epsilon: 1e-8

lr_scheduler:
  scheduler: "cosine"
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 100
    min_lr_scale: 0.1

training:
  gradient_accumulation_steps: 1
  batch_size: 96
  mixed_precision: "bf16"
  enable_tf32: true
  seed: 42
  max_train_steps: 300000
  max_grad_norm: 1.0
  lm_coeff: 1.0
  ignore_eos_in_masking: true

generation:
  max_new_tokens: 64
  num_steps: 64
  temperature: 0.7
  top_k: 50
  top_p: 0.95
  mask_schedule: "cosine"
  seed: 42
  strategy: "ratio"                 # options: ratio, fixed_tokens, official
  selection_scope: "all"            # ratio strategy: all or masked_only
  tokens_per_step: null             # fixed_tokens strategy: integer tokens per step
  block_length: null                # official strategy: must divide max_new_tokens if set
  remasking: "low_confidence"       # official strategy: low_confidence or random
  cfg_scale: 0.0                    # classifier-free guidance scale (official)
  logits_eos_inf: false             # official: set EOS logits to -inf
  confidence_eos_eot_inf: false     # official: suppress EOS/EOT confidences
  mask_token_id: null               # override mask token id (optional)
  forbid_logits_token_ids: null     # list of token ids to suppress in logits (official)
  forbid_confidence_token_ids: null # list of token ids to suppress in confidence (official)
