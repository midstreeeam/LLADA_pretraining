wandb:
  entity: null
  resume: auto
  run_id: 5qtqwctf
experiment:
  project: llada-tiny-test
  name: llada-tiny-test
  output_dir: outputs/tiny_test
  save_every: 2500
  eval_every: 100
  generate_every: 500
  log_every: 25
  log_grad_norm_every: 100
  checkpoints_total_limit: 3
  resume_from_checkpoint: outputs/tiny_test/checkpoint-20000
  logging_dir: outputs/tiny_test/logs
model:
  pretrained_model_path: gpt2
  llada_config:
    gradient_checkpointing: false
    d_model: 768
    n_heads: 12
    n_layers: 12
    n_kv_heads: 12
    mlp_ratio: 4
    max_sequence_length: 512
    vocab_size: 50257
    new_vocab_size: 50304
    rope: true
    rope_theta: 10000.0
    flash_attention: false
    attention_dropout: 0.0
    residual_dropout: 0.0
    embedding_dropout: 0.0
    layer_norm_type: rms
    activation_type: swiglu
    init_fn: normal
    init_std: 0.02
    weight_tying: true
    include_bias: false
dataset:
  params:
    train_shards_path_or_url: data/tinystories_train.jsonl
    shuffle_buffer_size: 500
    num_workers: 2
    pin_memory: true
    persistent_workers: false
  preprocessing:
    max_seq_length: 256
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    scale_lr: false
    beta1: 0.9
    beta2: 0.95
    weight_decay: 0.01
    epsilon: 1.0e-08
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 100
    min_lr_scale: 0.1
training:
  gradient_accumulation_steps: 8
  batch_size: 2
  mixed_precision: bf16
  enable_tf32: true
  seed: 42
  max_train_steps: 200000
  max_grad_norm: 1.0
  lm_coeff: 1.0
config: configs/tiny_test.yaml
